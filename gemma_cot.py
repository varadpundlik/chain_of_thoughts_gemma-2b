# -*- coding: utf-8 -*-
"""Gemma COT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_DvJ-FhkWRnephUC7kTVV5ZEeIMM4oA
"""

!pip install datasets transformers accelerate bitsandbytes peft trl

# core idea (not full script)
from datasets import load_dataset
ds = load_dataset("greengerong/leetcode")

ds

def make_example(example):
    title = example.get("title", "").strip()
    desc = example.get("content", "").strip()
    solution = example.get("python", "")  # field names may vary; inspect dataset first
    difficulty = example.get("difficulty", "")

    # Compose instruction (short + clean)
    short_desc = "\n".join(desc.splitlines()[0:3])
    instruction = f"Solve the problem: {title}. {short_desc}..."

    # Optional input field (examples, function signature, etc.)
    input_field = example.get("content", "")

    # ------------------------------
    # Elaborated Chain-of-Thought Template
    # ------------------------------
    cot = f"""Chain-of-Thought:

1. **Problem Understanding**
   The problem is titled "{title}".
   The description is: {short_desc}.
   We must understand the required input/output format and identify hidden constraints or edge cases.

2. **Key Observations**
   - Identify crucial patterns in the problem (e.g., sorting behavior, graph structure, DP relation, prefix/suffix properties).
   - Infer which constraints determine feasible complexity (O(n), O(n log n), etc.).
   - Detect typical pitfalls (off-by-one, integer overflow, duplicate handling, boundary cases).

3. **Reasoning & Approach Selection**
   - Consider all viable strategies.
   - Discard approaches that violate time or space constraints.
   - Justify the optimal algorithmic paradigm (e.g., hash map, two-pointer, BFS/DFS, DP, binary search, greedy).
   - Explain why the selected approach guarantees correctness.

4. **Algorithm Breakdown**
   - Provide a clear, ordered list of implementation steps.
   - Handle special cases separately (empty input, zero values, duplicates, etc.).
   - Present complexity analysis (time + space).

5. **Dry Run / Mini Simulation**
   - Walk through a representative or tricky example step-by-step.
   - Show how the algorithm transitions through states and reaches the correct output.
   - Confirm correctness and edge-case behavior.

6. **Final Implementation Notes**
   - Mention potential pitfalls to avoid in the final code.
   - Explain any important initialization or boundary conditions.
   - Ensure final code is concise, readable, and correct.

"""

    # Final answer / code
    final = solution if solution else "<write solution here>"

    output = cot + "\nFinal Answer (Python Code):\n```python\n" + final + "\n```"

    return {
        "instruction": instruction,
        "input": input_field,
        "output": output,
        "metadata": {"id": example.get("id"), "difficulty": difficulty}
    }

x=[]

#iterate over ds and append make_example output in X
# for i in range(len(ds["train"])):
#     x.append(make_example(ds["train"][i]))
# len(x)

for i in range(len(ds["train"])):
    x.append(make_example(ds["train"][i]))
len(x)

print(x[0]["instruction"])

print(x[0]["input"])

print(x[0]["output"])

!hf auth login

!pip uninstall -y bitsandbytes # Force uninstall bitsandbytes
!pip install -U bitsandbytes # Reinstall the latest bitsandbytes

import accelerate
!pip install -U bitsandbytes
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig # Import BitsAndBytesConfig
import torch # Import torch for dtype

model_name = "google/gemma-2-2b-it"   # or gemma-7b-it

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# Create a BitsAndBytesConfig object
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4", # Or "fp4"
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config, # Pass the quantization_config object
    device_map="auto",
    trust_remote_code=True
)

tokenizer.model_max_length = 1536
tokenizer.padding_side = "right"
tokenizer.truncation_side = "right"

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,                       # very important for T4
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj","o_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
gradient_checkpointing = True
model.gradient_checkpointing_enable()
# Enable gradient calculation for model inputs, crucial for gradient checkpointing with PEFT
model.enable_input_require_grads()
model.print_trainable_parameters()

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="gemma-leetcode-t4",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    learning_rate=3e-4,
    warmup_ratio=0.03,
    num_train_epochs=1,
    logging_steps=20,
    save_steps=5000,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="none",
    remove_unused_columns=False,
    gradient_checkpointing=True,  # important for T4
)

# define train_dataset and data_collator
from datasets import Dataset
from transformers import DataCollatorWithPadding

def preprocess_function(examples):
    # examples contains lists under keys "instruction", "input", "output"
    inputs = []
    labels = []

    # build user-only prompts and full chat prompts (vectorized per batch)
    user_prompts = []
    full_prompts = []
    for user_text, inp_text, assistant_text in zip(examples["instruction"],
                                                   examples["input"],
                                                   examples["output"]):
        user = user_text
        if inp_text:
            user = user + "\n\nInput Examples:\n" + inp_text
        # full chat: user + assistant
        chat = [
            {"role": "user", "content": user},
            {"role": "assistant", "content": assistant_text},
        ]

        # make the string prompt using Gemma chat template (no tokenization yet)
        full_prompt = tokenizer.apply_chat_template(chat, tokenize=False)
        # user-only prompt (so we can count tokens to mask labels)
        user_chat = [{"role": "user", "content": user}]
        user_prompt = tokenizer.apply_chat_template(user_chat, tokenize=False)

        full_prompts.append(full_prompt)
        user_prompts.append(user_prompt)

    # tokenize both lists in batch
    tokenized_full = tokenizer(
        full_prompts,
        truncation=True,
        max_length=1536,
        padding=False,
        return_tensors=None
    )

    tokenized_user = tokenizer(
        user_prompts,
        truncation=True,
        max_length=1536,
        padding=False,
        return_tensors=None
    )

    # build labels: copy input_ids, then mask user tokens with -100
    out = {}
    input_ids_list = tokenized_full["input_ids"]
    attention_mask_list = tokenized_full["attention_mask"]
    user_len_list = [len(u) for u in tokenized_user["input_ids"]]

    labels_list = []
    for idx, ids in enumerate(input_ids_list):
        # copy
        lab = ids.copy()
        # mask user tokens (first user_len_list[idx] tokens) to -100
        user_len = user_len_list[idx]
        # safety: ensure we don't exceed
        if user_len > len(lab):
            user_len = len(lab)
        for j in range(user_len):
            lab[j] = -100
        labels_list.append(lab)

    out["input_ids"] = input_ids_list
    out["attention_mask"] = attention_mask_list
    out["labels"] = labels_list
    return out

# Create the train_dataset (rehash your raw_train_dataset)
raw_train_dataset = Dataset.from_list(x)
train_dataset = raw_train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_train_dataset.column_names,
)

# use a simple padding collator (it will also pad labels properly)
data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=None)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

trainer.train()

from peft import PeftModel

# Save LoRA adapters only
trainer.model.save_pretrained("gemma-2b-qlora-cot-adapter")
tokenizer.save_pretrained("gemma-2b-qlora-cot-adapter")

from google.colab import files
uploaded = files.upload()

!unzip gemma-2b-cot-leetcode.zip

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_name = "google/gemma-2-2b-it"   # Changed to match the likely base model of the adapter
lora_path = "gemma-2b-cot-leetcode"

tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype="auto",
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, lora_path)
model.eval()

def generate(model,tokenizer,instruction,input_text):
  user = instruction
  if input_text:
        user = user + "\n\nInput:\n" + input_text

    # Provide a steering / instruction sentence (safe, short) in the user prompt
  steering = " Please provide a clear step-by-step reasoning labeled 'Step 1:', 'Step 2:', ... followed by the final answer."
  user = user + steering

  messages = [{"role": "user", "content": user}]
  inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
  ).to(model.device)

  outputs = model.generate(**inputs, max_new_tokens=5000)
  return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

# ----------------------------
# Inference on a random example
# ----------------------------
import random

idx = random.randint(0, len(x)-1)
test = x[idx]

print("\n⭐ TEST EXAMPLE:")
print(test["instruction"])

result = generate(
    model,
    tokenizer,
    instruction=test["instruction"],
    input_text=test["input"]
)

print("\n⭐ MODEL OUTPUT:")
print(result)

print("\n⭐ EXPECTED OUTPUT:")
print(test["output"])