# -*- coding: utf-8 -*-
"""Gemma COT Codeforces.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUREODMF9f2ysXQtt7u9CWO-Gnq5rEsK
"""

!pip install datasets transformers accelerate bitsandbytes peft trl

from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("open-r1/codeforces-cots", "solutions")

ds

ds['train'][0]['messages']

!pip uninstall -y bitsandbytes # Force uninstall bitsandbytes
!pip install -U bitsandbytes # Reinstall the latest bitsandbytes

import accelerate
!pip install -U bitsandbytes
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig # Import BitsAndBytesConfig
import torch # Import torch for dtype

model_name = "google/gemma-2-2b-it"   # or gemma-7b-it

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# Create a BitsAndBytesConfig object
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4", # Or "fp4"
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config, # Pass the quantization_config object
    device_map="auto",
    trust_remote_code=True
)

tokenizer.model_max_length = 1536
tokenizer.padding_side = "right"
tokenizer.truncation_side = "right"

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,                       # very important for T4
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj","o_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
gradient_checkpointing = True
model.gradient_checkpointing_enable()
# Enable gradient calculation for model inputs, crucial for gradient checkpointing with PEFT
model.enable_input_require_grads()
model.print_trainable_parameters()

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="gemma-codeforces-cot-t4",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    learning_rate=3e-4,
    warmup_ratio=0.03,
    num_train_epochs=1,
    logging_steps=20,
    save_steps=5000,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="none",
    remove_unused_columns=False,
    gradient_checkpointing=True,  # important for T4
)

# define train_dataset and data_collator
from datasets import Dataset
from transformers import DataCollatorWithPadding

def preprocess_function(batch):
    tokenized_outputs = {
        'input_ids': [],
        'labels': [],
        'attention_mask': []
    }

    # Iterate over each example (conversation history) in the batch
    # Fix: Iterate by index to handle batch['messages'] being a Dataset object
    for i in range(len(batch['messages'])):
        messages_list = batch['messages'][i]

        # 1. Tokenize the full conversation (user + assistant turns)
        full_tokenized_output = tokenizer.apply_chat_template(
            messages_list,
            truncation=True,
            max_length=1536,
            padding=False,
            return_tensors=None, # Return as list of integers for DataCollator
            tokenize=True,
            add_generation_prompt=False, # We want the entire conversation for training input
            return_dict=True
        )

        full_input_ids = full_tokenized_output['input_ids']
        full_attention_mask = full_tokenized_output['attention_mask']

        # 2. Determine the length of the "user" portion for masking labels.
        user_prefix_messages = []
        for msg in messages_list:
            if msg['role'] == 'assistant':
                break # Stop collecting messages once the assistant's turn starts
            user_prefix_messages.append(msg)

        user_part_for_masking_output = tokenizer.apply_chat_template(
            user_prefix_messages,
            truncation=True,
            max_length=1536,
            padding=False,
            return_tensors=None,
            tokenize=True,
            add_generation_prompt=True, # Include the prompt for assistant's generation
            return_dict=True
        )

        user_prompt_token_length = len(user_part_for_masking_output['input_ids'])

        labels = full_input_ids.copy()

        for j in range(min(user_prompt_token_length, len(labels))):
            labels[j] = -100

        tokenized_outputs['input_ids'].append(full_input_ids)
        tokenized_outputs['labels'].append(labels)
        tokenized_outputs['attention_mask'].append(full_attention_mask)

    return tokenized_outputs

# Split the dataset into train and test
ds_split = ds['train'].train_test_split(train_size=1000, test_size=200)


train_dataset = ds_split['train'].map(
    preprocess_function,
    batched=True,
    remove_columns=ds['train'].column_names, # Remove original columns if desired
)

eval_dataset = ds_split['test'].map(
    preprocess_function,
    batched=True,
    remove_columns=ds['train'].column_names, # Remove original columns if desired
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

trainer.train()

from peft import PeftModel

# Save LoRA adapters only
trainer.model.save_pretrained("gemma-2b-codeforces-cot-adapter")
tokenizer.save_pretrained("gemma-2b-codeforces-cot-adapter")

from google.colab import files
uploaded = files.upload()

!unzip gemma-2b-cot-leetcode.zip

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_name = "google/gemma-2-2b-it"   # Changed to match the likely base model of the adapter
lora_path = "gemma-2b-cot-leetcode"

tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype="auto",
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, lora_path)
model.eval()

import torch

def generate(model, tokenizer, prompt,
             max_new_tokens=512,
             temperature=0.2,
             top_p=0.95,
             do_sample=False,
             num_return_sequences=1):
    """
    Robust generation wrapper:
    - moves input tensors to the model device,
    - uses no_grad,
    - sets pad_token_id/eos_token_id when needed,
    - decodes result cleanly.
    """
    # Ensure model on eval
    model.eval()

    # Build chat messages as you used elsewhere
    messages = [{"role": "user", "content": prompt}]
    tokenized = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    )

    # tokenized is a dict like {'input_ids': tensor(...), 'attention_mask': tensor(...)}
    # Move only tensors to the model device
    model_device = next(model.parameters()).device
    for k, v in tokenized.items():
        if isinstance(v, torch.Tensor):
            tokenized[k] = v.to(model_device)

    input_ids = tokenized["input_ids"]
    attention_mask = tokenized.get("attention_mask", None)

    # safety: ensure pad_token_id/eos_token_id for generation
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token or tokenizer.eos_token_id
    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id
    eos_id = tokenizer.eos_token_id

    gen_kwargs = dict(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        do_sample=do_sample,
        num_return_sequences=num_return_sequences,
        pad_token_id=pad_id,
        eos_token_id=eos_id,
    )

    with torch.no_grad():
        outputs = model.generate(**gen_kwargs)

    # outputs shape: (num_return_sequences, seq_len)
    results = []
    # start index for generated tokens
    start_idx = input_ids.shape[-1]
    for i in range(outputs.shape[0]):
        gen_ids = outputs[i][start_idx:]
        text = tokenizer.decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        results.append(text)

    # if only one sequence requested, return the string
    if num_return_sequences == 1:
        return results[0]
    return results

# ----------------------------
# Inference on a random example
# ----------------------------
import random

idx = random.randint(1001, len(ds['train'])-1)
test = ds['train'][idx]

print("\n⭐ TEST EXAMPLE:")
print(test["prompt"])

result = generate(model, tokenizer, prompt=test['prompt'], max_new_tokens=5000, temperature=0.1, do_sample=False)

print("\n⭐ MODEL OUTPUT:")
print(result)

print("\n⭐ EXPECTED OUTPUT:")
print(test["generation"])